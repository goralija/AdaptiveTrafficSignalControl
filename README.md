# Adaptivno upravljanje saobraÄ‡ajnom signalizacijom pomoÄ‡u Q-learninga

Ovaj repozitorij sadrÅ¾i programski kod i konfiguracione fajlove koriÅ¡tene u okviru zavrÅ¡nog rada na ElektrotehniÄkom fakultetu u Sarajevu.  
Cilj projekta je implementacija **Q-learning agenta** koji adaptivno upravlja semaforima na raskrsnici, u svrhu smanjenja guÅ¾vi i poboljÅ¡anja protoÄnosti saobraÄ‡aja.

---

## ğŸ¯ Cilj i motivacija
Tradicionalni sistemi semafora koriste unaprijed definisane fiksne planove, Å¡to u realnim uslovima dovodi do zaguÅ¡enja i poveÄ‡anog vremena Äekanja.  
KoriÅ¡tenjem **reinforcement learninga (pojaÄanog uÄenja)** moguÄ‡e je razviti agenta koji se prilagoÄ‘ava trenutnom stanju u saobraÄ‡aju i donosi bolje odluke u realnom vremenu.

---

## ğŸ§  Metodologija
- **Algoritam:** Q-learning (model-free RL).  
- **OkruÅ¾enje:** SUMO (Simulation of Urban Mobility) simulator + TraCI API za interakciju u realnom vremenu.  
- **Predstavljanje stanja:**  
  - Broj vozila po traci (diskretizovan u binove od 5 vozila, do max. 60).  
  - Trajanje trenutne faze semafora (diskretizovano u intervale od 10 sekundi).  
- **Akcije:**  
  1. ZadrÅ¾i trenutnu fazu semafora  
  2. Promijeni fazu semafora  
- **Funkcija nagrade:** negativna vrijednost kvadratne sume duÅ¾ina redova Äekanja + penalizacija za neefikasne faze.  
- **Proces treniranja:** preko 2000 epizoda simulacije, sa periodiÄnim Äuvanjem Q-tabela i logova.  
- **Evaluacija:** poreÄ‘enje sa fiksnim semaforskim planom pomoÄ‡u metrika prosjeÄnog Äekanja i ukupnog vremena trajanja simulacije.

---

## ğŸ“Š Rezultati
- RL agent znaÄajno smanjuje prosjeÄno vrijeme Äekanja vozila u poreÄ‘enju sa fiksnim planovima.  
- Eksperimenti pokazuju poboljÅ¡anje protoÄnosti i smanjenje zaguÅ¡enja, Äak i uz jednostavnu implementaciju Q-learninga.  
- Logovi i Q-tabele omoguÄ‡avaju praÄ‡enje procesa uÄenja i analizu konvergencije.

---

## ğŸ“‚ Struktura repozitorija

```
.
â”œâ”€â”€ README.md # Ovaj fajl
â”œâ”€â”€ requirements.txt # Python zavisnosti
â”‚
â”œâ”€â”€ simulation-config/ # SUMO konfiguracija
â”‚ â”œâ”€â”€ osm.net.xml.gz # SaobraÄ‡ajna mreÅ¾a (kompresovana)
â”‚ â”œâ”€â”€ osm.netccfg # Network konfiguracija
â”‚ â”œâ”€â”€ osm.poly.xml.gz # Poligoni (objekti okruÅ¾enja)
â”‚ â”œâ”€â”€ osm.polycfg # Konfiguracija poligona
â”‚ â”œâ”€â”€ osm.sumocfg # Glavni konfiguracioni fajl simulacije
â”‚ â”œâ”€â”€ osm.view.xml # PodeÅ¡avanja vizualizacije
â”‚ â””â”€â”€ osm_bbox.osm.xml.gz # OSM ulazni fajl za simulaciju
â”‚
â””â”€â”€ src/ # Python kod
â”œâ”€â”€ config.py # Konfiguracija hiperparametara i simulacije
â”œâ”€â”€ evaluate_agent.py # Evaluacija nauÄenog modela
â”œâ”€â”€ run_training.py # Glavna skripta za trening
â””â”€â”€ utils.py # PomoÄ‡ne funkcije
```

---

## âš™ï¸ Instalacija i pokretanje

### 1. Instalacija zavisnosti
Potrebno je imati **Python 3.9+** i instaliran **SUMO** simulator.  
Instalacija Python zavisnosti:
```bash
pip install -r requirements.txt
```

Instalacija SUMO-a:  
ğŸ‘‰ [Uputstvo](https://sumo.dlr.de/docs/Installing.html)

### 2. Pokretanje treninga agenta
```bash
python src/run_training.py
```

### 3. Evaluacija nauÄenog modela
```bash
python src/evaluate_agent.py
```

---

## ğŸš€ BuduÄ‡i rad
- ProÅ¡irenje na **viÅ¡e raskrsnica** (multi-agent RL).  
- Optimizacija **funkcije nagrade** (ukljuÄivanje protoka i kaÅ¡njenja).  
- Ispitivanje naprednijih RL algoritama: **DQN, DDPG, PPO**.  
- Automatska optimizacija hiperparametara.  

---

## ğŸ“š Reference
- Abdulhai, B., Pringle, R., Karakoulas, G. J. (2003). *Reinforcement learning for true adaptive traffic signal control*.  
- Yau, K.-L. A., et al. (2017). *A survey on reinforcement learning models and algorithms for traffic signal control*.  
- Balaji, P., German, X., Srinivasan, D. (2010). *Urban traffic signal control using reinforcement learning agents*.  

---

âœï¸ Autor: **Harun Goralija**  
ğŸ“… 2025
